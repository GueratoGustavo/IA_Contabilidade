{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "INFO:__main__:scikit-learn: 1.7.0\n",
      "INFO:__main__:Dataset carregado com 525000 linhas e 20 colunas.\n",
      "INFO:__main__:Iniciando GridSearchCV...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "# --- Instalar dependências (executar no notebook se necessário)\n",
    "!pip install pandas matplotlib seaborn scikit-learn joblib --quiet\n",
    "\n",
    "# --- Importações principais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "# --- Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Python: {sys.version}\")\n",
    "logger.info(f\"scikit-learn: {sys.modules['sklearn'].__version__}\")\n",
    "\n",
    "# --- 1. Carregar dados\n",
    "DATA_PATH = \"../data/processed/training_data.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "logger.info(f\"Dataset carregado com {df.shape[0]} linhas e {df.shape[1]} colunas.\")\n",
    "\n",
    "# --- 2. Converter datas e criar feature derivada prazo_dias\n",
    "for col in [\"data_emissao\", \"data_vencimento\"]:\n",
    "    df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "df[\"prazo_dias\"] = (df[\"data_vencimento\"] - df[\"data_emissao\"]).dt.days\n",
    "\n",
    "# --- 3. Função para extrair features simples de texto (corrigida)\n",
    "def extract_text_features(df):\n",
    "    # Garantir que as colunas são string e sem NaN para uso do .str\n",
    "    df[\"descricao\"] = df[\"descricao\"].astype(str).fillna(\"\")\n",
    "    df[\"observacao\"] = df[\"observacao\"].astype(str).fillna(\"\")\n",
    "\n",
    "    df[\"tem_nota_fiscal\"] = df[\"descricao\"].str.contains(\"nota fiscal\", case=False, na=False).astype(int)\n",
    "    df[\"tem_imposto\"] = df[\"descricao\"].str.contains(\"imposto\", case=False, na=False).astype(int)\n",
    "    df[\"tem_boleto\"] = df[\"descricao\"].str.contains(\"boleto\", case=False, na=False).astype(int)\n",
    "    df[\"tem_recurso_juridico\"] = df[\"observacao\"].str.contains(\"recurso\", case=False, na=False).astype(int)\n",
    "    return df\n",
    "\n",
    "df = extract_text_features(df)\n",
    "\n",
    "# --- 4. Definir target e colunas a remover (excluir texto original e possíveis vazamentos)\n",
    "TARGET_COL = \"classificacao\"\n",
    "drop_cols = [\n",
    "    \"id\", \"nome_emissor\", \"cnpj_emissor\", \"cpf_responsavel\", \"numero_documento\",\n",
    "    \"raiz_cnpj\", \"filial_cnpj\", \"dv_cnpj\", \"descricao\", \"observacao\",\n",
    "    \"data_emissao\", \"data_vencimento\",\n",
    "    \"categoria\", \"status\", \"tipo_documento\"\n",
    "]\n",
    "\n",
    "# --- 5. Remover classes raras para stratify\n",
    "y_counts = df[TARGET_COL].value_counts()\n",
    "valid_classes = y_counts[y_counts >= 5].index\n",
    "df = df[df[TARGET_COL].isin(valid_classes)]\n",
    "\n",
    "# --- 6. Separar X e y\n",
    "X = df.drop(columns=[col for col in drop_cols if col in df.columns] + [TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# --- 7. Converter tipos e preencher NaNs nas colunas numéricas (incluindo as criadas)\n",
    "numerical_cols = [\n",
    "    \"valor_total\", \"numero_paginas\", \"confianca_esperada\", \"prazo_dias\",\n",
    "    \"tem_nota_fiscal\", \"tem_imposto\", \"tem_boleto\", \"tem_recurso_juridico\"\n",
    "]\n",
    "\n",
    "for col in numerical_cols:\n",
    "    X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].median())\n",
    "\n",
    "# Garantir ausência de NaNs finais\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        X[col] = X[col].fillna(0)\n",
    "assert not X.isnull().any().any(), \"Ainda há valores ausentes em X!\"\n",
    "\n",
    "# --- 8. Preprocessor e pipeline com RandomForest e class_weight balanceado\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numerical_cols)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# --- 9. Separar treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 10. Configurar GridSearchCV para ajustar hiperparâmetros importantes\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [10, 15, 20],\n",
    "    \"classifier__min_samples_leaf\": [1, 3, 5],\n",
    "    \"classifier__class_weight\": [\"balanced\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- 11. Treinar GridSearch\n",
    "logger.info(\"Iniciando GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "logger.info(f\"Melhores parâmetros: {grid_search.best_params_}\")\n",
    "logger.info(f\"Melhor F1 Macro (validação): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# --- 12. Avaliar no conjunto teste\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Acurácia: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-score macro: {f1_score(y_test, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "# --- 13. Matriz de confusão detalhada\n",
    "plt.figure(figsize=(8,6))\n",
    "ConfusionMatrixDisplay.from_estimator(grid_search, X_test, y_test, cmap=\"Blues\", xticks_rotation=45)\n",
    "plt.title(\"Matriz de Confusão - Teste\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 14. Importância das features (do melhor modelo)\n",
    "best_rf = grid_search.best_estimator_.named_steps[\"classifier\"]\n",
    "feature_names = numerical_cols\n",
    "importances = best_rf.feature_importances_\n",
    "importances_df = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances_df.values, y=importances_df.index)\n",
    "plt.title(\"Importância das Features - Melhor Modelo\")\n",
    "plt.xlabel(\"Importância\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 15. Salvar melhor modelo pipeline\n",
    "MODEL_PATH = \"../models/rf_pipeline_best.pkl\"\n",
    "joblib.dump(grid_search.best_estimator_, MODEL_PATH)\n",
    "logger.info(f\"Melhor modelo salvo em {MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
